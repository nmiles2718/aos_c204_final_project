{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edde769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import random # to set the python random seed\n",
    "import sys\n",
    "import time\n",
    "# This should map to the root directory of the github repo\n",
    "# if it doesnt, can set it manaully\n",
    "ROOT_DIR = os.path.dirname(os.getcwd())\n",
    "print(ROOT_DIR)\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "# This should map to the data directory of the github repo\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data','sta_chunks')\n",
    "print(DATA_DIR)\n",
    "\n",
    "import numpy as np # to set the numpy random seed\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as TF\n",
    "import torchvision.transforms.functional as TVF\n",
    "from torchmetrics.functional import accuracy\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "# Ignore excessive warnings\n",
    "import logging\n",
    "logging.propagate = False \n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# WandB â€“ Import the wandb library\n",
    "# import wandb\n",
    "from pytorch_lightning import LightningModule, seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Local imports\n",
    "from utils.icme_dataset import ICMEDataset\n",
    "from utils.icme_net import IcmeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f0e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7930881",
   "metadata": {},
   "source": [
    "Function used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e31b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, \n",
    "    criterion,\n",
    "    optimizer, \n",
    "    scheduler,\n",
    "    num_epochs=25,\n",
    "    dataloaders=None,\n",
    "    device=None\n",
    "):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    dataset_sizes = {}\n",
    "    for key in dataloaders.keys():\n",
    "        dataset_sizes[key] = len(dataloaders[key])\n",
    "    \n",
    "    training_data = {'train_loss':[], 'val_loss':[], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase], desc=phase, total=len(dataloaders[phase])):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs.float())\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            training_data[f'{phase}_loss'].append(epoch_loss)\n",
    "            training_data[f'{phase}_acc'].append(epoch_acc)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "              \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "icme_train_dataset = ICMEDataset(\n",
    "    'sta_train_set.txt', \n",
    "    rootdir=ROOT_DIR,\n",
    "    datadir=DATA_DIR\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    icme_train_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Test dataset and loader\n",
    "icme_test_dataset = ICMEDataset(\n",
    "    'sta_test_set.txt', \n",
    "    rootdir=ROOT_DIR,\n",
    "    datadir=DATA_DIR\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    icme_test_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "#val dataset and loader\n",
    "icme_val_dataset = ICMEDataset(\n",
    "    'sta_validation_set.txt', \n",
    "    rootdir=ROOT_DIR,\n",
    "    datadir=DATA_DIR\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    icme_val_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166def04",
   "metadata": {},
   "source": [
    "Collect the training and validation loaders to use during the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'train': train_loader, 'val':val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19990de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IcmeNet(num_classes=2, kernel_size=9, train_loader=train_loader, test_loader=test_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6f177",
   "metadata": {},
   "source": [
    "Use GPU if it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd805a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b4c59",
   "metadata": {},
   "source": [
    "Define the optimizer, learning rate scheduler, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b535190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use stochastic gradient descent as our optimization routine\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# We will also use a scheduler for the learning rate.\n",
    "# This allows us to optimize the learning rate by having large steps\n",
    "# at first and then getting gradually smaller (by a factor of 1/2) every 5 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = TF.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd337d55",
   "metadata": {},
   "source": [
    "Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b7c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, training_data = train_model(\n",
    "    model, \n",
    "    criterion=loss_fn, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,\n",
    "    num_epochs=20,\n",
    "    dataloaders=dataloaders,\n",
    "    device=device\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277bd3d",
   "metadata": {},
   "source": [
    "Proceed with the following cells if you are content with the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df = pd.DataFrame(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42dd4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df['train_acc'] = [val.cpu().numpy() for val in training_data_df['train_acc']]\n",
    "training_data_df['val_acc'] = [val.cpu().numpy() for val in training_data_df['val_acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8549f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df.to_csv('./gdrive/MyDrive/aos_c205_final_project_data/training_results.txt', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4938b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '../adam_optim_neural_network.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
